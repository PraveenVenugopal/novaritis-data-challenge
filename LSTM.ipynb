{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from keras import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "# from imblearn.keras import BalancedBatchGenerator\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import  multilabel_confusion_matrix\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")\n",
    "train_labels = pd.read_csv(\"data/train_labels.csv\",header=None).values.reshape(-1)\n",
    "test_labels = pd.read_csv(\"data/test_labels.csv\",header=None).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper fucntion to convert labels into OHE vectors\n",
    "def convert_to_cat(labels):\n",
    "    enc = LabelEncoder()\n",
    "    enc_y = enc.fit_transform(labels)\n",
    "    categorical_y = np_utils.to_categorical(enc_y)\n",
    "    print(categorical_y.shape,categorical_y[:2])\n",
    "    return categorical_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65809, 132) (65809, 23)\n",
      "(43201, 132) (43201, 23)\n"
     ]
    }
   ],
   "source": [
    "# Separate categorical fields from numeric fields\n",
    "\n",
    "cat_cols = [k for k in list(train_data.columns) if train_data[k].dtype != \"float64\"]\n",
    "num_cols = list(set(list(train_data.columns))-set(cat_cols))\n",
    "\n",
    "# Construct disjoint datasets for cateorical and numeric types\n",
    "categorical_data = train_data[cat_cols].copy()\n",
    "numeric_data = train_data[num_cols].copy()\n",
    "print(categorical_data.shape,numeric_data.shape)\n",
    "\n",
    "categorical_test_data = test_data[cat_cols].copy()\n",
    "numeric_test_data = test_data[num_cols].copy()\n",
    "print(categorical_test_data.shape,numeric_test_data.shape)\n",
    "\n",
    "\n",
    "# Have a copy of the merged, complete dataset. We will use both these datasets in further steps \n",
    "train_data = train_data.values\n",
    "test_data = test_data.values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape  (109010, 155)\n",
      "Reduced  (109010, 2)\n",
      "(65809, 2) (43201, 2)\n"
     ]
    }
   ],
   "source": [
    "# If need be, feed the reduced dimensional data from PCA to the LSTM\n",
    "# Just another experiment that was tried, tested and failed\n",
    "from sklearn.decomposition import PCA\n",
    "total_X = np.concatenate([train_data,test_data])\n",
    "\n",
    "print(\"Original Shape \",total_X.shape)\n",
    "pca = PCA(n_components=2,svd_solver=\"full\")\n",
    "total_X = pca.fit_transform(total_X)\n",
    "print(\"Reduced \",total_X.shape)\n",
    "\n",
    "train_x = total_X[:train_data.shape[0]]\n",
    "test_x = total_X[train_data.shape[0]:]\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65809, 22) (65809, 132)\n",
      "(65809, 154)\n",
      "(43201, 22) (43201, 132)\n",
      "(43201, 154)\n"
     ]
    }
   ],
   "source": [
    "# Remove a specific numeric columns from our training pipeline.\n",
    "# Here we use the disjoint datasets we created two cells ago.\n",
    "\n",
    "drop_col = [\"MODSTS.552051\"] #,\"VI552051.748\"]\n",
    "\n",
    "for col in drop_col:\n",
    "    numeric_data.drop([col],inplace=True,axis=1)\n",
    "    numeric_test_data.drop([col],inplace=True,axis=1)\n",
    "    print(numeric_data.shape,categorical_data.shape)\n",
    "    train_data = pd.concat([numeric_data,categorical_data],axis=1).values\n",
    "    print(train_data.shape)\n",
    "    \n",
    "    print(numeric_test_data.shape,categorical_test_data.shape)\n",
    "    test_data = pd.concat([numeric_test_data,categorical_test_data],axis=1).values\n",
    "    print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adadelta, Adam\n",
    "from keras.losses import mean_squared_error,mean_absolute_error, logcosh, categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from keras.initializers import RandomNormal, RandomUniform, glorot_normal\n",
    "from keras.callbacks import TerminateOnNaN, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "\n",
    "# Parameters for Early stopping\n",
    "cb1 = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, verbose=0, mode='auto', restore_best_weights=False)\n",
    "cb2 = TerminateOnNaN()\n",
    "cb = [cb1,cb2]\n",
    "\n",
    "# Setting Hyperparameters -  Though custom packages like AutoKeras perform search space construction,\n",
    "# they dont specifically address time-series data requirements. We proceed with our own setting\n",
    "\n",
    "more_optim = []\n",
    "adam_beta = [[0.9,0.999],[0.8,0.888],[0.7,0.777]]\n",
    "lr = [0.1,0.01,0.001]\n",
    "for l in lr:\n",
    "    more_optim.append(SGD(lr=l))\n",
    "    more_optim.append(Adadelta(lr=l))\n",
    "    for b in adam_beta:\n",
    "        more_optim.append(Adam(lr=l,beta_1=b[0],beta_2=b[1]))\n",
    "\n",
    "# A conveinient overwrite option if the above setting seems highly resource intensive\n",
    "optim = [\"sgd\",\"adadelta\",\"adam\"]\n",
    "loss = [\"categorical_crossentropy\"] #,\"sparse_categorical_crossentropy\"]\n",
    "init = ['glorot_uniform','random_uniform','random_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constructing the Vanilla LSTM here\n",
    "\n",
    "n_steps = 6\n",
    "n_features = train_data.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "# Embeddings are not useful in this setting\n",
    "#model.add(Embedding(124,32,input_length=155))\n",
    "\n",
    "#model.add(LSTM(units=124,activation='relu',return_sequences=True))\n",
    "#model.add(LSTM(units=64, activation='relu',return_sequences=True))\n",
    "model.add(LSTM(units=16,activation='relu',input_shape=(n_steps,n_features)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crucial to training this function convers sequential data \n",
    "# to \"batch data with time_steps\" to feed in to LSTM \n",
    "\n",
    "def process_batch(data,lbls,time_steps=3):\n",
    "    dt = []\n",
    "    labels = []\n",
    "    i = time_steps\n",
    "    while i < len(data):\n",
    "        dt.append(data[i-time_steps:i])\n",
    "        labels.append(lbls[i])\n",
    "        i +=1  \n",
    "    return np.asarray(dt),np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65809, 3) [[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(65803, 6, 154) (65803, 3)\n",
      "[0.12628954 0.12628954 0.12628954 ... 0.12628954 0.12628954 0.12628954]\n"
     ]
    }
   ],
   "source": [
    "# Hardcoding here with the best parameters found so far\n",
    "\n",
    "batch_size = 32\n",
    "n_loss = \"categorical_crossentropy\"\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "if n_loss == \"categorical_crossentropy\":\n",
    "    categorical_y = convert_to_cat(train_labels)\n",
    "    train_in,train_lbl = process_batch(train_data,categorical_y,n_steps)\n",
    "else:\n",
    "    train_in,train_lbl = process_batch(train_data,train_labels,n_steps)\n",
    "\n",
    "print(train_in.shape,train_lbl.shape)\n",
    "n_samples = len(train_in)\n",
    "d_sample_weight = class_weight.compute_sample_weight('balanced',train_lbl)\n",
    "print(d_sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52642 samples, validate on 13161 samples\n",
      "Epoch 1/5\n",
      "52642/52642 [==============================] - 18s 339us/step - loss: 495.5447 - acc: 0.0044 - categorical_accuracy: 0.0044 - val_loss: 2.0323 - val_acc: 0.0016 - val_categorical_accuracy: 0.0016\n",
      "Epoch 2/5\n",
      "52642/52642 [==============================] - 17s 326us/step - loss: 495.5447 - acc: 0.0044 - categorical_accuracy: 0.0044 - val_loss: 2.0323 - val_acc: 0.0016 - val_categorical_accuracy: 0.0016\n",
      "Epoch 3/5\n",
      "52642/52642 [==============================] - 15s 294us/step - loss: 495.5447 - acc: 0.0044 - categorical_accuracy: 0.0044 - val_loss: 2.0323 - val_acc: 0.0016 - val_categorical_accuracy: 0.0016\n",
      "Epoch 4/5\n",
      "52642/52642 [==============================] - 16s 297us/step - loss: 495.5447 - acc: 0.0044 - categorical_accuracy: 0.0044 - val_loss: 2.0323 - val_acc: 0.0016 - val_categorical_accuracy: 0.0016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let us run the model on the best parameters that I tuned.\n",
    "model.compile(loss=n_loss, optimizer=opt,metrics=[\"accuracy\",\"categorical_accuracy\"])\n",
    "history = model.fit(train_in,train_lbl,epochs=5,validation_split=0.2,sample_weight=d_sample_weight,batch_size=batch_size,callbacks=cb)\n",
    "\n",
    "\n",
    "# If you wish to try out the hyperparameter setting we set with \"adam\",\"sgd\" etc, uncomment this section \n",
    "# and run it. Spolier Alert : Highly resource intensive\n",
    "\n",
    "# for opt in more_optim:\n",
    "#     print(\" OPT \",t)\n",
    "#     for ls in loss:\n",
    "#         print(opt,ls,\"------------------------------\")\n",
    "#         model.compile(loss=ls, optimizer=opt,metrics=[\"accuracy\",\"categorical_accuracy\"])\n",
    "#         history = model.fit(train_in,train_lbl,epochs=20,validation_split=0.2,sample_weight=d_sample_weight,batch_size=batch_size,callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43201, 3) [[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "43191/43191 [==============================] - 7s 155us/step\n",
      "Accuracy: 99.34%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('complete_model.h5') \n",
    "del model  \n",
    "\n",
    "categorical_y = convert_to_cat(test_labels)\n",
    "test_in,test_lbl = process_batch(test_data,categorical_y,time_steps)\n",
    "\n",
    "results = model.evaluate(test_in, test_lbl)\n",
    "predictions = model.predict(test_in)\n",
    "print(\"Accuracy: %.2f%%\" % (results[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.argmax(1)\n",
    "print(predictions[:2])\n",
    "print(test_lbl[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42907    52     0]\n",
      " [   43     0     0]\n",
      " [  189     0     0]]\n"
     ]
    }
   ],
   "source": [
    "lbl = test_lbl.argmax(1)\n",
    "print(confusion_matrix(lbl,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
